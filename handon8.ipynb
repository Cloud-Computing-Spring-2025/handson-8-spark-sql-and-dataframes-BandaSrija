{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afrqbUjAKh0T",
        "outputId": "b0dc2ced-0075-4617-ae57-1245dd68192f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset generation complete: 'users.csv' and 'posts.csv' created in /input/\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Create input directory if it doesn't exist\n",
        "os.makedirs(\"input\", exist_ok=True)\n",
        "\n",
        "# Sample users\n",
        "user_data = []\n",
        "usernames = [\n",
        "    \"@techie42\", \"@critic99\", \"@daily_vibes\", \"@designer_dan\", \"@rage_user\",\n",
        "    \"@meme_lord\", \"@social_queen\", \"@calm_mind\", \"@pixel_pusher\", \"@stream_bot\"\n",
        "]\n",
        "age_groups = [\"Teen\", \"Adult\", \"Senior\"]\n",
        "countries = [\"US\", \"UK\", \"Canada\", \"India\", \"Germany\", \"Brazil\"]\n",
        "verified_status = [True, False]\n",
        "\n",
        "for user_id in range(1, 9):\n",
        "    user = {\n",
        "        \"UserID\": user_id,\n",
        "        \"Username\": usernames[user_id - 1],\n",
        "        \"AgeGroup\": random.choice(age_groups),\n",
        "        \"Country\": random.choice(countries),\n",
        "        \"Verified\": random.choice(verified_status)\n",
        "    }\n",
        "    user_data.append(user)\n",
        "\n",
        "# Write users.csv\n",
        "with open(\"input/users.csv\", mode=\"w\", newline=\"\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=user_data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(user_data)\n",
        "\n",
        "# Sample posts\n",
        "hashtags_pool = [\"#tech\", \"#fail\", \"#design\", \"#UX\", \"#cleanUI\", \"#mood\", \"#bug\", \"#love\", \"#social\", \"#AI\"]\n",
        "contents = [\n",
        "    \"Loving the new update!\",\n",
        "    \"This app keeps crashing. So annoying.\",\n",
        "    \"Just another day...\",\n",
        "    \"Absolutely love the UX!\",\n",
        "    \"Worst experience ever.\",\n",
        "    \"Such a smooth interface!\",\n",
        "    \"Great performance on mobile.\",\n",
        "    \"Can’t stop using it!\",\n",
        "    \"Needs dark mode ASAP!\",\n",
        "    \"I’m impressed with the speed.\"\n",
        "]\n",
        "\n",
        "posts_data = []\n",
        "base_time = datetime.now()\n",
        "\n",
        "for post_id in range(101, 201):\n",
        "    uid = random.randint(1, 10)\n",
        "    timestamp = (base_time - timedelta(hours=random.randint(0, 240))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    content = random.choice(contents)\n",
        "    likes = random.randint(0, 150)\n",
        "    retweets = random.randint(0, 50)\n",
        "    sentiment = round(random.uniform(-1, 1), 2)\n",
        "    hashtags = \",\".join(random.sample(hashtags_pool, random.randint(1, 3)))\n",
        "\n",
        "    post = {\n",
        "        \"PostID\": post_id,\n",
        "        \"UserID\": uid,\n",
        "        \"Content\": content,\n",
        "        \"Timestamp\": timestamp,\n",
        "        \"Likes\": likes,\n",
        "        \"Retweets\": retweets,\n",
        "        \"Hashtags\": hashtags,\n",
        "        \"SentimentScore\": sentiment\n",
        "    }\n",
        "    posts_data.append(post)\n",
        "\n",
        "# Write posts.csv\n",
        "with open(\"input/posts.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=posts_data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(posts_data)\n",
        "\n",
        "print(\"✅ Dataset generation complete: 'users.csv' and 'posts.csv' created in /input/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "posts_df = pd.read_csv(\"input/posts.csv\")\n",
        "\n",
        "# Split Hashtags and expand into individual tags\n",
        "hashtags = posts_df['Hashtags'].str.split(',', expand=True).stack()\n",
        "\n",
        "# Count frequency of each hashtag\n",
        "hashtag_count = hashtags.value_counts().reset_index()\n",
        "hashtag_count.columns = ['Hashtag', 'Count']\n",
        "\n",
        "# Get the top 10 most used hashtags\n",
        "top_hashtags = hashtag_count.head(10)\n",
        "print(top_hashtags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH5uSd1SN6a5",
        "outputId": "a1741817-610f-459d-9c31-493fe236c577"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Hashtag  Count\n",
            "0     #tech     27\n",
            "1  #cleanUI     26\n",
            "2     #love     25\n",
            "3      #bug     23\n",
            "4     #mood     21\n",
            "5       #UX     20\n",
            "6       #AI     15\n",
            "7   #design     15\n",
            "8     #fail     15\n",
            "9   #social     13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "posts_df = pd.read_csv(\"input/posts.csv\")\n",
        "users_df = pd.read_csv(\"input/users.csv\")\n",
        "\n",
        "# Merge posts with users on UserID\n",
        "merged_df = pd.merge(posts_df, users_df, on=\"UserID\")\n",
        "\n",
        "# Group by AgeGroup and calculate average likes and retweets\n",
        "age_group_engagement = merged_df.groupby('AgeGroup').agg(\n",
        "    avg_likes=('Likes', 'mean'),\n",
        "    avg_retweets=('Retweets', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Sort by average engagement (likes + retweets)\n",
        "age_group_engagement['total_engagement'] = age_group_engagement['avg_likes'] + age_group_engagement['avg_retweets']\n",
        "age_group_engagement = age_group_engagement.sort_values('total_engagement', ascending=False)\n",
        "\n",
        "print(age_group_engagement[['AgeGroup', 'avg_likes', 'avg_retweets']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJg5wqutOHDN",
        "outputId": "a5290153-c612-49fd-8de6-da9a7614b0bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AgeGroup  avg_likes  avg_retweets\n",
            "2     Teen  79.500000     30.388889\n",
            "1   Senior  64.782609     28.086957\n",
            "0    Adult  67.351351     23.729730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "posts_df = pd.read_csv(\"input/posts.csv\")\n",
        "\n",
        "# Categorize Sentiment\n",
        "def categorize_sentiment(score):\n",
        "    if score > 0.2:\n",
        "        return 'Positive'\n",
        "    elif score < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "posts_df['SentimentCategory'] = posts_df['SentimentScore'].apply(categorize_sentiment)\n",
        "\n",
        "# Group by sentiment category and calculate average likes and retweets\n",
        "sentiment_engagement = posts_df.groupby('SentimentCategory').agg(\n",
        "    avg_likes=('Likes', 'mean'),\n",
        "    avg_retweets=('Retweets', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(sentiment_engagement)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV5ICZSNOO4x",
        "outputId": "50090a9a-c329-4a98-97d0-6b7bdf424aad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  SentimentCategory  avg_likes  avg_retweets\n",
            "0          Negative  66.978723     23.787234\n",
            "1           Neutral  75.750000     23.375000\n",
            "2          Positive  71.378378     28.351351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "posts_df = pd.read_csv(\"input/posts.csv\")\n",
        "users_df = pd.read_csv(\"input/users.csv\")\n",
        "\n",
        "# Merge posts with users on UserID\n",
        "merged_df = pd.merge(posts_df, users_df, on=\"UserID\")\n",
        "\n",
        "# Filter verified users\n",
        "verified_users = merged_df[merged_df['Verified'] == True].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Calculate total reach as Likes + Retweets using .loc[]\n",
        "verified_users.loc[:, 'TotalReach'] = verified_users['Likes'] + verified_users['Retweets']\n",
        "\n",
        "# Group by Username and sum the total reach\n",
        "user_reach = verified_users.groupby('Username').agg(total_reach=('TotalReach', 'sum')).reset_index()\n",
        "\n",
        "# Get top 5 users by total reach\n",
        "top_verified_users = user_reach.sort_values('total_reach', ascending=False).head(5)\n",
        "\n",
        "print(top_verified_users)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO7yvwL4OPlH",
        "outputId": "15f77dae-1015-4deb-da47-ad93b05f32a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Username  total_reach\n",
            "2  @daily_vibes         1314\n",
            "3    @rage_user         1055\n",
            "1     @critic99         1031\n",
            "0    @calm_mind         1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, col, trim\n",
        "\n",
        "# ✅ Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HashtagTrends\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ✅ Set log level to hide warnings\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# ✅ Bypass Windows-specific Hadoop native IO errors\n",
        "spark._jsc.hadoopConfiguration().set(\"hadoop.home.dir\", \"C:/hadoop\")\n",
        "spark._jsc.hadoopConfiguration().set(\"dfs.client.use.datanode.hostname\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.file.impl.disable.cache\", \"true\")\n",
        "spark._jsc.hadoopConfiguration().set(\"hadoop.native.lib\", \"false\")\n",
        "\n",
        "# ✅ Load posts data\n",
        "posts_df = spark.read.option(\"header\", True).csv(\"input/posts.csv\")\n",
        "\n",
        "# ✅ Handle empty or missing hashtags\n",
        "posts_df = posts_df.filter(col(\"Hashtags\").isNotNull())\n",
        "\n",
        "# ✅ Split the 'Hashtags' column and explode it to create individual rows\n",
        "hashtag_df = posts_df.withColumn(\"Hashtag\", explode(split(col(\"Hashtags\"), \",\")))\n",
        "\n",
        "# ✅ Remove leading/trailing spaces from hashtags\n",
        "hashtag_df = hashtag_df.withColumn(\"Hashtag\", trim(col(\"Hashtag\")))\n",
        "\n",
        "# ✅ Count the frequency of each hashtag\n",
        "hashtag_counts = hashtag_df.groupBy(\"Hashtag\").count()\n",
        "\n",
        "# ✅ Sort by frequency in descending order\n",
        "hashtag_counts = hashtag_counts.orderBy(col(\"count\").desc())\n",
        "\n",
        "# ✅ Write result safely\n",
        "hashtag_counts.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"outputs/hashtag_trends.csv\")\n",
        "\n",
        "# ✅ Optional: Print top 10 to console\n",
        "print(\"Top hashtags:\")\n",
        "hashtag_counts.show(10)\n",
        "\n",
        "# ✅ Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "M5j0oZh6OTXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8692bfcc-1758-487b-a29e-c321f4547415"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top hashtags:\n",
            "+--------+-----+\n",
            "| Hashtag|count|\n",
            "+--------+-----+\n",
            "|   #tech|   27|\n",
            "|#cleanUI|   26|\n",
            "|   #love|   25|\n",
            "|    #bug|   23|\n",
            "|   #mood|   21|\n",
            "|     #UX|   20|\n",
            "|     #AI|   15|\n",
            "| #design|   15|\n",
            "|   #fail|   15|\n",
            "| #social|   13|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EngagementByAgeGroup\").getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "posts_df = spark.read.option(\"header\", True).csv(\"input/posts.csv\", inferSchema=True)\n",
        "users_df = spark.read.option(\"header\", True).csv(\"input/users.csv\", inferSchema=True)\n",
        "\n",
        "# Join the datasets on UserID\n",
        "joined_df = posts_df.join(users_df, on=\"UserID\", how=\"inner\")\n",
        "\n",
        "# Group by AgeGroup and calculate average likes and retweets\n",
        "engagement_df = joined_df.groupBy(\"AgeGroup\").agg(\n",
        "    avg(\"Likes\").alias(\"Avg Likes\"),\n",
        "    avg(\"Retweets\").alias(\"Avg Retweets\")\n",
        ")\n",
        "\n",
        "# Sort by Avg Likes (you can also sort by Avg Retweets if needed)\n",
        "engagement_df = engagement_df.orderBy(col(\"Avg Likes\").desc())\n",
        "\n",
        "# Save result\n",
        "engagement_df.coalesce(1).write.mode(\"overwrite\").csv(\"outputs/engagement_by_age.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "fFOKVtOp3I4c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, avg, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SentimentVsEngagement\").getOrCreate()\n",
        "\n",
        "# Load posts data\n",
        "posts_df = spark.read.option(\"header\", True).csv(\"input/posts.csv\", inferSchema=True)\n",
        "\n",
        "# Categorize posts into Positive, Neutral, and Negative sentiment\n",
        "posts_df = posts_df.withColumn(\n",
        "    \"SentimentCategory\",\n",
        "    when(col(\"SentimentScore\") > 0.3, \"Positive\")\n",
        "    .when(col(\"SentimentScore\") < -0.3, \"Negative\")\n",
        "    .otherwise(\"Neutral\")\n",
        ")\n",
        "\n",
        "# Calculate average likes and retweets per sentiment category\n",
        "sentiment_stats = posts_df.groupBy(\"SentimentCategory\").agg(\n",
        "    avg(\"Likes\").alias(\"Avg Likes\"),\n",
        "    avg(\"Retweets\").alias(\"Avg Retweets\")\n",
        ")\n",
        "\n",
        "# Save result\n",
        "sentiment_stats.coalesce(1).write.mode(\"overwrite\").csv(\"outputs/sentiment_engagement.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "-dfBQuMK3SmT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TopVerifiedUsers\").getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "posts_df = spark.read.option(\"header\", True).csv(\"input/posts.csv\", inferSchema=True)\n",
        "users_df = spark.read.option(\"header\", True).csv(\"input/users.csv\", inferSchema=True)\n",
        "\n",
        "# Join datasets on UserID and filter for verified users\n",
        "verified_users_df = posts_df.join(users_df.filter(col(\"Verified\") == True), on=\"UserID\", how=\"inner\")\n",
        "\n",
        "# Calculate total reach (Likes + Retweets) for each verified user\n",
        "user_reach_df = verified_users_df.groupBy(\"Username\").agg(\n",
        "    (_sum(\"Likes\") + _sum(\"Retweets\")).alias(\"Total Reach\")\n",
        ")\n",
        "\n",
        "# Sort by Total Reach in descending order\n",
        "top_verified = user_reach_df.orderBy(col(\"Total Reach\").desc()).limit(5)\n",
        "\n",
        "# Save result\n",
        "top_verified.coalesce(1).write.mode(\"overwrite\").csv(\"outputs/top_verified_users.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "2qnVqWNV3dIr"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}